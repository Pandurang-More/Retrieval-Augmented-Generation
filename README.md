# ğŸ“„ Offline RAG Document Summarizer

A fully **offline Retrieval-Augmented Generation (RAG)** system for summarizing
research papers and documents using **FAISS** for retrieval and **Ollama** for
local LLM inference.  
ğŸš« No OpenAI key â€¢ ğŸš« No cloud dependency â€¢ âœ… Privacy-friendly

---

## âœ¨ Features

- ğŸ” Semantic retrieval using **FAISS**
- ğŸ§© **Semantic chunking** for better context understanding
- ğŸ¯ **Cross-encoder reranking** to improve summary quality
- ğŸ¤– **Local LLM inference using Ollama (Mistral / LLaMA)**
- ğŸ“„ PDF upload and summarization
- ğŸ–¥ï¸ Interactive **Streamlit UI**
- âš ï¸ Handles empty / scanned PDF edge cases
- ğŸš€ Optimized with caching for faster repeated runs

---

## ğŸ§  System Architecture

PDF â†’ Text Extraction  
â†’ Semantic Chunking  
â†’ FAISS Vector Index  
â†’ Reranking (Cross-Encoder)  
â†’ Ollama Local LLM  
â†’ Summary Output  

---

## ğŸ› ï¸ Tech Stack

- Python
- FAISS
- LangChain
- Sentence-Transformers
- Ollama (Local LLM)
- PyMuPDF
- Streamlit

---

## ğŸ“‚ Project Structure

```
RAG/
â”œâ”€â”€ app.py            # Streamlit UI
â”œâ”€â”€ ingest.py         # PDF loading & chunking
â”œâ”€â”€ rag_pipeline.py   # Retrieval + reranking + LLM
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ venv/             # ignored
```

---

## ğŸš€ How to Run This Project (Local)

### 1ï¸âƒ£ Prerequisites

- Python 3.10+
- Git
- Ollama installed  
  ğŸ‘‰ https://ollama.com/download

---

### 2ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/rag-summarizer.git
cd rag-summarizer
```

---

### 3ï¸âƒ£ Create & Activate Virtual Environment

```bash
python -m venv venv
```

**Windows**
```powershell
venv\Scripts\activate
```

**Linux / macOS**
```bash
source venv/bin/activate
```

---

### 4ï¸âƒ£ Install Dependencies

```bash
python -m pip install -r requirements.txt
```

---

### 5ï¸âƒ£ Start Ollama (Local LLM)

Open a separate terminal and run:

```bash
ollama run mistral
```

âš ï¸ Keep this terminal running.

---

### 6ï¸âƒ£ Run the Streamlit App

```bash
streamlit run app.py
```

Open in browser:
```
http://localhost:8501
```

---

## ğŸ“Œ Usage

1. Upload a PDF (research paper or document)
2. Click **Generate Summary**
3. View AI-generated summary using a **local LLM**

---

## âš ï¸ Notes

- This project runs **fully offline**
- Ollama must be running locally
- Scanned/image-only PDFs require OCR (not included)

---

## ğŸ“ˆ Future Enhancements

- OCR support for scanned PDFs
- Page-level citations
- Multi-document summarization
- GPU acceleration
- Optional cloud deployment

---


